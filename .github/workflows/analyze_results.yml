# Analyze UMD benchmark results
name: Analyze benchmark results

on:
  workflow_call:
    inputs:
      arch:
        required: true
        type: string
      ubuntu-docker-version:
        required: true
        type: string
      card:
        required: true
        type: string
      baseline-run-id:
        required: false
        type: string
  workflow_dispatch:
    inputs:
      arch:
        required: true
        type: string
      ubuntu-docker-version:
        required: true
        type: string
      card:
        required: true
        type: string
      results-run-id:
        required: true
        type: string
      baseline-run-id:
        required: false
        type: string

env:
  UMD_MICROBENCHMARK_RESULTS_PATH: /tmp/umd_microbenchmark_results

jobs:
  analyze:
    name: Analyze results for ${{ inputs.arch }} on ${{ inputs.card }} on ${{ inputs.ubuntu-docker-version }}
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository }}/tt-umd-ci-${{ inputs.ubuntu-docker-version }}:latest

    steps:
      - name: Checkout analysis tools
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            tests/microbenchmark/tools/analyze_results.py
          sparse-checkout-cone-mode: false

      - name: Create and activate virtual environment
        run: |
          python3 -m venv /tmp/venv
          . /tmp/venv/bin/activate
          echo "PATH=$PATH" >> $GITHUB_ENV
          echo "VIRTUAL_ENV=$VIRTUAL_ENV" >> $GITHUB_ENV

      - name: Install python dependencies
        run: |
          . /tmp/venv/bin/activate
          python3 -m pip install --no-cache-dir pandas psutil tabulate

      - name: Install dependencies for API calls
        run: |
          apt-get update -qq
          apt-get install -y --no-install-recommends curl jq gh

      - name: Create benchmark results directory
        run: mkdir -p ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/{base,compare}

      - name: Get latest main run ID for "Build and run all benchmarks"
        id: get-main-run
        if: inputs.baseline-run-id == ''
        continue-on-error: true
        run: |
          MAIN_RUN_ID=$(gh run list -b main -w 175973905 -s success -L 1 --json databaseId --jq '.[0].databaseId')
          echo Found: $MAIN_RUN_ID
          echo "run-id=$MAIN_RUN_ID" >> $GITHUB_OUTPUT

      - name: Download baseline JSON benchmark results
        id: download-baseline
        if: steps.get-main-run.outcome == 'success'
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: benchmark-json-${{ inputs.arch }}-${{ inputs.card }}-${{ inputs.ubuntu-docker-version }}
          run-id: ${{ inputs.baseline-run-id != '' && inputs.baseline-run-id || steps.get-main-run.outputs.run-id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/base

      - name: Download comparison JSON benchmark results
        id: download-comparison
        uses: actions/download-artifact@v4
        continue-on-error: false
        with:
          name: benchmark-json-${{ inputs.arch }}-${{ inputs.card }}-${{ inputs.ubuntu-docker-version }}
          path: ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/compare

      - name: Run analysis of benchmark data (with baseline)
        if: steps.download-baseline.outcome == 'success' && steps.download-comparison.outcome == 'success'
        run: |
          . /tmp/venv/bin/activate
          echo "Architecture: ${{ inputs.arch }}" > ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md
          echo "Card: ${{ inputs.card }}" >> ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md
          echo "" >> ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md
          python3 tests/microbenchmark/tools/analyze_results.py \
          ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/compare -c ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/base \
          | tee ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md

      - name: Run analysis of benchmark data (comparison only)
        if: steps.download-baseline.outcome != 'success' && steps.download-comparison.outcome == 'success'
        run: |
          . /tmp/venv/bin/activate
          echo "Architecture: ${{ inputs.arch }}" > ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md
          echo "Card: ${{ inputs.card }}" >> ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md
          echo "" >> ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md
          python3 tests/microbenchmark/tools/analyze_results.py \
          ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/compare \
          | tee -a ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/results.md

      - name: Cleanup benchmark results directory
        if: always()
        run: rm -rf ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}
