# Analyze UMD benchmark results
name: Analyze benchmark results

on:
  workflow_call:
    inputs:
      arch:
        required: true
        type: string
      ubuntu-docker-version:
        required: true
        type: string
      card:
        required: true
        type: string
      baseline-run-id:
        required: false
        type: string


env:
  UMD_MICROBENCHMARK_RESULTS_PATH: /tmp/umd_microbenchmark_results

jobs:
  analyze:
    name: Analyze results for ${{ inputs.arch }} on ${{ inputs.card }} on ${{ inputs.ubuntu-docker-version }}
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository }}/tt-umd-ci-${{ inputs.ubuntu-docker-version }}:latest

    steps:
      - name: Checkout analysis tools
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            tests/microbenchmark/tools/analyze_results.py
          sparse-checkout-cone-mode: false

      - name: Install python dependencies
        run: python3 -m pip install --no-cache-dir pandas psutil tabulate

      - name: Install dependencies for API calls
        run: |
          apt-get update -qq
          apt-get install -y --no-install-recommends curl jq

      - name: Create benchmark results directory
        run: mkdir -p ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/{base,compare}

      - name: Get latest main run ID for "Build and run all benchmarks"
        id: get-main-run
        if: inputs.baseline-run-id == ''
        continue-on-error: true
        run: |
          MAIN_RUN_ID=$(curl -s -H "Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/runs?branch=main&status=success&per_page=1" \
            | jq -r '.workflow_runs[] | select(.name == "Build and run all benchmarks") | .id' | head -1)
          echo "run-id=$MAIN_RUN_ID" >> $GITHUB_OUTPUT

      - name: Download baseline JSON benchmark results
        id: download-baseline
        if: steps.get-main-run.outcome == 'success'
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: benchmark-json-${{ inputs.arch }}-${{ inputs.card }}-${{ inputs.ubuntu-docker-version }}
          run-id: ${{ inputs.baseline-run-id != '' && inputs.baseline-run-id || steps.get-main-run.outputs.run-id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/base

      - name: Download comparison JSON benchmark results
        id: download-comparison
        uses: actions/download-artifact@v4
        continue-on-error: false
        with:
          name: benchmark-json-${{ inputs.arch }}-${{ inputs.card }}-${{ inputs.ubuntu-docker-version }}
          path: ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/compare

      - name: Run analysis of benchmark data (with baseline)
        if: steps.download-baseline.outcome == 'success' && steps.download-comparison.outcome == 'success'
        run: |
          python3 tests/microbenchmark/tools/analyze_results.py \
          ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/compare -c ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/base

      - name: Run analysis of benchmark data (comparison only)
        if: steps.download-baseline.outcome != 'success' && steps.download-comparison.outcome == 'success'
        run: |
          python3 tests/microbenchmark/tools/analyze_results.py \
          ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}/compare

      - name: Cleanup benchmark results directory
        if: always()
        run: rm -rf ${{ env.UMD_MICROBENCHMARK_RESULTS_PATH }}
