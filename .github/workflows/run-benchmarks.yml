# Run UMD benchmarks on the specified architecture and card, on all supported OS versions.
name: Run UMD benchmarks

on:
  workflow_call:
    inputs:
      arch:
        required: true
        type: string
      ubuntu-docker-version:
        required: true
        type: string
      card:
        required: true
        type: string
      timeout:
        required: true
        type: number
      build-type:
        required: true
        type: string
        default: Release

env:
  BUILD_OUTPUT_DIR: ./build
  TEST_OUTPUT_DIR: ./build/test
  CREATE_MAP_BINARIES_DIR: ./device/bin/silicon
  TT_UMD_LOGGER_LEVEL: info

jobs:
  test:
    # Due to parsing bug, fromJSON is used to convert string to number
    timeout-minutes: ${{ fromJSON(inputs.timeout) }}

    name: Run benchmarks for ${{ inputs.arch }} on ${{ inputs.card }} on ${{ inputs.ubuntu-docker-version }}
    runs-on:
      - ${{ inputs.card }}
    container:
      image: ghcr.io/${{ github.repository }}/tt-umd-ci-${{ inputs.ubuntu-docker-version }}:latest
      # Set dummy var for baremetal, as options has to have some value
      options: ${{ inputs.arch != 'baremetal' && '--device /dev/tenstorrent' || '-e DUMMY_VAR=' }}
      volumes:
        - /dev/hugepages:/dev/hugepages
        - /dev/hugepages-1G:/dev/hugepages-1G
        - /etc/udev/rules.d:/etc/udev/rules.d
        - /lib/modules:/lib/modules
        - /opt/tt_metal_infra/provisioning/provisioning_env:/opt/tt_metal_infra/provisioning/provisioning_env

    env:
      LD_LIBRARY_PATH: ./build/lib
      UMD_MICROBENCHMARK_RESULTS_PATH: /tmp/umd_microbenchmark_results

    steps:
      - name: Cleanup tt-umd dir, and change directory as if we were in a github.repository
        run: |
          rm -rf tt-umd
          mkdir tt-umd
          cd tt-umd

      - name: Use build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts-${{ inputs.ubuntu-docker-version }}-${{ inputs.build-type }}
          path: ./

      # This is needed to preserve file permissions
      # https://github.com/actions/upload-artifact?tab=readme-ov-file#permission-loss
      - name: 'Untar build artifacts'
        shell: bash
        run: tar xvf artifact.tar

      - name: Checkout gather_host_specs.py tool
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            tests/microbenchmark/tools/gather_host_specs.py
          sparse-checkout-cone-mode: false
          path: tt-umd

      - name: Install python dependencies
        run: python3 -m pip install --no-cache-dir pandas psutil tabulate

      - name: Create benchmark results directory
        run: mkdir -p /tmp/umd_microbenchmark_results

      - name: Gather host machine specification
        if: ${{ inputs.arch != 'baremetal' }}
        run: |
            python3 $GITHUB_WORKSPACE/tt-umd/tests/microbenchmark/tools/gather_host_specs.py \
            --json --output $UMD_MICROBENCHMARK_RESULTS_PATH/machine_host_spec.json

      - name: Run TLB benchmarks
        if: ${{ inputs.arch != 'baremetal' }}
        run: |
          ${{ env.TEST_OUTPUT_DIR }}/umd/microbenchmark/umd_microbenchmark  --gtest_filter=MicrobenchmarkTLB*

      - name: Run PCIe benchmarks
        if: ${{ inputs.arch != 'baremetal' }}
        run: |
          ${{ env.TEST_OUTPUT_DIR }}/umd/microbenchmark/umd_microbenchmark  --gtest_filter=MicrobenchmarkPCIeDMA*

      - name: Run IOMMU benchmarks
        if: ${{ inputs.arch != 'baremetal' }}
        run: |
          ${{ env.TEST_OUTPUT_DIR }}/umd/microbenchmark/umd_microbenchmark  --gtest_filter=MicrobenchmarkIOMMU*

      - name: Run Cluster open benchmarks
        if: ${{ inputs.arch != 'baremetal' }}
        run: |
          ${{ env.TEST_OUTPUT_DIR }}/umd/microbenchmark/umd_microbenchmark  --gtest_filter=MicrobenchmarkOpenCluster*

      - name: Run ethernet IO benchmarks
        if: ${{ inputs.arch != 'baremetal' }}
        run: |
          ${{ env.TEST_OUTPUT_DIR }}/umd/microbenchmark/umd_microbenchmark  --gtest_filter=MicrobenchmarkEthernetIO*

      - name: Upload JSON benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-json-${{ inputs.arch }}-${{ inputs.card }}-${{ inputs.ubuntu-docker-version }}
          path: /tmp/umd_microbenchmark_results/*.json
          if-no-files-found: warn

      - name: Upload HTML benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-html-${{ inputs.arch }}-${{ inputs.card }}-${{ inputs.ubuntu-docker-version }}
          path: /tmp/umd_microbenchmark_results/*.html
          if-no-files-found: warn

      - name: Cleanup benchmark results directory
        run: rm -rf /tmp/umd_microbenchmark_results
